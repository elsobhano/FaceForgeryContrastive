{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5445dcb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086380e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43753edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dllabsharif/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch import Tensor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from torchvision import transforms, datasets\n",
    "\n",
    "# from util import AverageMeter\n",
    "# from util import adjust_learning_rate, warmup_learning_rate\n",
    "# from util import set_optimizer, save_model\n",
    "# from networks.resnet_big import SupConResNe\n",
    "from losses import SupConLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d0c33",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4591a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = 100\n",
    "save_freq = 100\n",
    "batch_size = 64\n",
    "epochs = 400\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "lr_decay_epochs = \"100,200,300\"\n",
    "lr_decay_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# Dataset\n",
    "dataset = 'faceforensic'\n",
    "\n",
    "temp = 0.07\n",
    "\n",
    "#method\n",
    "method = 'SupCon'\n",
    "\n",
    "cosine = True\n",
    "warm = True\n",
    "trial = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a467d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './save/SupCon/{}_models'.format(dataset)\n",
    "tb_path = './save/SupCon/{}_tensorboard'.format(dataset)\n",
    "lr_decay_epochs = [int(it) for it in lr_decay_epochs.split(',')]\n",
    "\n",
    "\n",
    "save_time = str(datetime.datetime.now())\n",
    "model_name = '{}_{}_{}_{}_lr_{}_decay_{}_bsz_{}_temp_{}_trial_{}'.\\\n",
    "        format(save_time, method, dataset, 'resnet50', learning_rate,\n",
    "               weight_decay, batch_size, temp, trial)\n",
    "\n",
    "if cosine:\n",
    "    model_name = '{}_cosine'.format(model_name)\n",
    "\n",
    "\n",
    "tb_folder = os.path.join(tb_path, model_name)\n",
    "if not os.path.isdir(tb_folder):\n",
    "    os.makedirs(tb_folder)\n",
    "\n",
    "save_folder = os.path.join(model_path, model_name)\n",
    "if not os.path.isdir(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9ffa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "\n",
    "class AverageMeter():\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def accuracy_evaluate(output, target, topk=(1,)):\n",
    "    \"\"\"accuarcy for evaluation 3+1\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        correct = []\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        pred = pred.view(-1)\n",
    "        # print(pred[5])\n",
    "        # print(target)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # print(pred[i].is_nonzero.eval())\n",
    "            # print(target[i].is_nonzero.eval())\n",
    "\n",
    "            if pred[i] == 0 and target[i] == 0:\n",
    "                correct.append(True)\n",
    "                tn += 1\n",
    "            elif pred[i] != 0 and target[i] != 0:\n",
    "                correct.append(True)\n",
    "                tp += 1\n",
    "            else:\n",
    "                if pred[i] == 0 and target[i] != 0:\n",
    "                    fn += 1\n",
    "                elif pred[i] != 0 and target[i] == 0:\n",
    "                    fp += 1\n",
    "                correct.append(False)\n",
    "\n",
    "        # print(correct)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = sum(bool(x) for x in correct)\n",
    "            # print(correct_k)\n",
    "            res.append(correct_k * 100.0 / batch_size)\n",
    "\n",
    "        return res\n",
    "\n",
    "def output_score(output, target):\n",
    "    print(output)\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if cosine:\n",
    "        eta_min = lr * (lr_decay_rate ** 3)\n",
    "        lr = eta_min + (lr - eta_min) * (\n",
    "                1 + math.cos(math.pi * epoch / epochs)) / 2\n",
    "    else:\n",
    "        steps = np.sum(epoch > np.asarray(lr_decay_epochs))\n",
    "        if steps > 0:\n",
    "            lr = lr * (lr_decay_rate ** steps)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def warmup_learning_rate(epoch, batch_id, total_batches, optimizer):\n",
    "    if warm and epoch <= warm_epochs:\n",
    "        p = (batch_id + (epoch - 1) * total_batches) / \\\n",
    "            (warm_epochs * total_batches)\n",
    "        lr = warmup_from + p * (warmup_to - warmup_from)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def set_optimizer(model):\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=learning_rate,\n",
    "                          momentum=momentum,\n",
    "                          weight_decay=weight_decay)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adamax(model.parameters(),  lr=1e-3, eps=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, save_file):\n",
    "    print('==> Saving...')\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, save_file)\n",
    "    del state\n",
    "\n",
    "\n",
    "class LinearClassifierFeatureFusion(nn.Module):\n",
    "    \"\"\"Linear classifier\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(LinearClassifierFeatureFusion, self).__init__()\n",
    "        feat_dim = 3328\n",
    "        self.fc1 = nn.Linear(feat_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = features.view(features.shape[0], -1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca36d9",
   "metadata": {},
   "source": [
    "## Splitting Train, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76a14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('train_idx.txt') == False:\n",
    "    total_idx = list(range(999))\n",
    "    random.shuffle(total_idx)\n",
    "    train_idx = total_idx[:600]\n",
    "\n",
    "    with open('train_idx.txt', 'w') as file:\n",
    "        json.dump(train_idx, file)\n",
    "\n",
    "    valid_idx = total_idx[600:800]\n",
    "\n",
    "    with open('valid_idx.txt', 'w') as file:\n",
    "        json.dump(valid_idx, file)\n",
    "\n",
    "\n",
    "    test_idx = total_idx[800:]\n",
    "\n",
    "    with open('test_idx.txt', 'w') as file:\n",
    "        json.dump(test_idx, file)\n",
    "\n",
    "else:\n",
    "    \n",
    "    with open('train_idx.txt', 'r') as file:\n",
    "        train_idx = json.load(file)\n",
    "        \n",
    "    with open('valid_idx.txt', 'r') as file:\n",
    "        valid_idx = json.load(file)\n",
    "        \n",
    "    with open('test_idx.txt', 'r') as file:\n",
    "        test_idx = json.load(file)\n",
    "        \n",
    "def idx_to_path(cat, indexes):\n",
    " \n",
    "    root = './Dataset/c23'\n",
    "    data_frame = {'path':[]}\n",
    "    path = root + '/{}'.format(cat)\n",
    "    for idx in indexes:\n",
    "        folder_path = path + '/{}'.format(idx)\n",
    "        if os.path.exists(folder_path) == True:\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = folder_path + '/{}'.format(file_name)\n",
    "                data_frame['path'].append(file_path)\n",
    "\n",
    "    if cat == 'Original':\n",
    "        labels = [1]*len(data_frame['path'])\n",
    "    else:\n",
    "        labels = [0]*len(data_frame['path'])\n",
    "\n",
    "    data_frame['labels'] = labels\n",
    "    data_frame = pd.DataFrame(data_frame)\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1908b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_data = idx_to_path('Deepfakes', train_idx)\n",
    "DF_valid_data = idx_to_path('Deepfakes', valid_idx)\n",
    "DF_test_data = idx_to_path('Deepfakes', test_idx)\n",
    "\n",
    "F2F_train_data = idx_to_path('Face2Face', train_idx)\n",
    "F2F_valid_data = idx_to_path('Face2Face', valid_idx)\n",
    "F2F_test_data = idx_to_path('Face2Face', test_idx)\n",
    "\n",
    "FS_train_data = idx_to_path('FaceSwap', train_idx)\n",
    "FS_valid_data = idx_to_path('FaceSwap', valid_idx)\n",
    "FS_test_data = idx_to_path('FaceSwap', test_idx)\n",
    "\n",
    "NT_train_data = idx_to_path('NeuralTextures', train_idx)\n",
    "NT_valid_data = idx_to_path('NeuralTextures', valid_idx)\n",
    "NT_test_data = idx_to_path('NeuralTextures', test_idx)\n",
    "\n",
    "OR_train_data = idx_to_path('Original', train_idx)\n",
    "OR_valid_data = idx_to_path('Original', valid_idx)\n",
    "OR_test_data = idx_to_path('Original', test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba065336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997\n",
      "17550\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_train_data))\n",
    "print(len(F2F_train_data))\n",
    "print(len(FS_train_data))\n",
    "print(len(OR_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0aed304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "5880\n",
      "6000\n",
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_valid_data))\n",
    "print(len(F2F_valid_data))\n",
    "print(len(FS_valid_data))\n",
    "print(len(NT_valid_data))\n",
    "print(len(OR_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d78e9c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5970\n",
      "5730\n",
      "5970\n",
      "5970\n",
      "5970\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_test_data))\n",
    "print(len(F2F_test_data))\n",
    "print(len(FS_test_data))\n",
    "print(len(NT_test_data))\n",
    "print(len(OR_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb9d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "#         transforms.RandomApply([\n",
    "#             transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "#         ], p=0.8),\n",
    "#         transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_frame, transform=None):\n",
    "        \n",
    "        self.data = data_frame\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.data.loc[idx, 'path']\n",
    "        label = self.data.loc[idx, 'labels']\n",
    "        # RGB\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f7420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_dataset = MyDataset(DF_train_data, TwoCropTransform(train_transform))\n",
    "DF_valid_dataset = MyDataset(DF_valid_data, TwoCropTransform(train_transform))\n",
    "DF_test_dataset = MyDataset(DF_test_data, TwoCropTransform(train_transform))\n",
    "\n",
    "F2F_train_dataset = MyDataset(F2F_train_data, TwoCropTransform(train_transform))\n",
    "F2F_valid_dataset = MyDataset(F2F_valid_data, TwoCropTransform(train_transform))\n",
    "F2F_test_dataset = MyDataset(F2F_test_data, TwoCropTransform(train_transform))\n",
    "\n",
    "FS_train_dataset = MyDataset(FS_train_data, TwoCropTransform(train_transform))\n",
    "FS_valid_dataset = MyDataset(FS_valid_data, TwoCropTransform(train_transform))\n",
    "FS_test_dataset = MyDataset(FS_test_data, TwoCropTransform(train_transform))\n",
    "\n",
    "NT_train_dataset = MyDataset(NT_train_data, TwoCropTransform(train_transform))\n",
    "NT_valid_dataset = MyDataset(NT_valid_data, TwoCropTransform(train_transform))\n",
    "NT_test_dataset = MyDataset(NT_test_data, TwoCropTransform(train_transform))\n",
    "\n",
    "OR_train_dataset = MyDataset(OR_train_data, TwoCropTransform(train_transform))\n",
    "OR_valid_dataset = MyDataset(OR_valid_data, TwoCropTransform(train_transform))\n",
    "OR_test_dataset = MyDataset(OR_test_data, TwoCropTransform(train_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef281d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_train_loader = torch.utils.data.DataLoader(DF_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# DF_valid_loader = torch.utils.data.DataLoader(DF_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# DF_test_loader = torch.utils.data.DataLoader(DF_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# F2F_train_loader = torch.utils.data.DataLoader(F2F_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# F2F_valid_loader = torch.utils.data.DataLoader(F2F_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# F2F_test_loader = torch.utils.data.DataLoader(F2F_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# FS_train_loader = torch.utils.data.DataLoader(FS_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# FS_valid_loader = torch.utils.data.DataLoader(FS_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# FS_test_loader = torch.utils.data.DataLoader(FS_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# NT_train_loader = torch.utils.data.DataLoader(NT_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# NT_valid_loader = torch.utils.data.DataLoader(NT_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# NT_test_loader = torch.utils.data.DataLoader(NT_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# OR_train_loader = torch.utils.data.DataLoader(OR_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# OR_valid_loader = torch.utils.data.DataLoader(OR_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "# OR_test_loader = torch.utils.data.DataLoader(OR_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8994ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'resnet18': 512,\n",
    "    'resnet34': 512,\n",
    "    'resnet50': 2048,\n",
    "    'resnet101': 2048\n",
    "}\n",
    "class SupConResNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, name='resnet50', head='mlp', feat_dim=128):\n",
    "        super(SupConResNet, self).__init__()\n",
    "        dim_in = model_dict[name]\n",
    "        img_model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(img_model.children())[:-1])\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(feat.shape[0], feat.shape[1])\n",
    "#         print(feat.shape)\n",
    "        feat = F.normalize(self.head(feat), dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c47ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inp = torch.randn((64,3,224,224))\n",
    "# test_model = SupConResNet()\n",
    "# test_out = test_model(test_inp)\n",
    "# print(test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69388c",
   "metadata": {},
   "source": [
    "## SRM Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f84268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMConv2d_simple(nn.Module):\n",
    "    \n",
    "    def __init__(self, inc=3, learnable=False):\n",
    "        super(SRMConv2d_simple, self).__init__()\n",
    "        self.truc = nn.Hardtanh(-3, 3)\n",
    "        kernel = self._build_kernel(inc)  # (3,3,5,5)\n",
    "        self.kernel = nn.Parameter(data=kernel, requires_grad=learnable)\n",
    "        # self.hor_kernel = self._build_kernel().transpose(0,1,3,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: imgs (Batch, H, W, 3)\n",
    "        '''\n",
    "        out = F.conv2d(x, self.kernel, stride=1, padding=2)\n",
    "        out = self.truc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _build_kernel(self, inc):\n",
    "        # filter1: KB\n",
    "        filter1 = [[0, 0, 0, 0, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 2, -4, 2, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 0, 0, 0, 0]]\n",
    "        # filter2：KV\n",
    "        filter2 = [[-1, 2, -2, 2, -1],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-2, 8, -12, 8, -2],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-1, 2, -2, 2, -1]]\n",
    "        # filter3：hor 2rd\n",
    "        filter3 = [[0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 1, -2, 1, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0]]\n",
    "\n",
    "        filter1 = np.asarray(filter1, dtype=float) / 4.\n",
    "        filter2 = np.asarray(filter2, dtype=float) / 12.\n",
    "        filter3 = np.asarray(filter3, dtype=float) / 2.\n",
    "        # statck the filters\n",
    "        filters = [[filter1],#, filter1, filter1],\n",
    "                   [filter2],#, filter2, filter2],\n",
    "                   [filter3]]#, filter3, filter3]]  # (3,3,5,5)\n",
    "        filters = np.array(filters)\n",
    "        filters = np.repeat(filters, inc, axis=1)\n",
    "        filters = torch.FloatTensor(filters)    # (3,3,5,5)\n",
    "        return filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba669cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConEffNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, head='mlp', feat_dim=128):\n",
    "        super(SupConEffNet, self).__init__()\n",
    "        # _, dim_in = model_dict[name]\n",
    "        self.srm = SRMConv2d_simple()\n",
    "        dim_in = 1280\n",
    "        self.encoder = EfficientNet.from_name('efficientnet-b0', num_classes=2, include_top=False)\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.srm(x)\n",
    "        x = self.encoder(x)\n",
    "#         print(feat.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         print(feat.shape)\n",
    "        x = F.normalize(self.head(x), dim=1)\n",
    "#         print(feat.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear classifier\"\"\"\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        feat_dim = 1280\n",
    "        self.fc1 = nn.Linear(feat_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = features.view(features.shape[0], -1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae8a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConEffNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, head='mlp', feat_dim=128):\n",
    "        super(SupConEffNet, self).__init__()\n",
    "        # _, dim_in = model_dict[name]\n",
    "#         self.srm = SRMConv2d_simple()\n",
    "        dim_in = 1280\n",
    "        self.encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        self.encoder.classifier = nn.Identity()\n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.srm(x)\n",
    "        x = self.encoder(x)\n",
    "#         print(feat.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         print(feat.shape)\n",
    "        x = F.normalize(self.head(x), dim=1)\n",
    "#         print(feat.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f5bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConEffNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\n",
    "    + Pre Stem Blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, head='mlp', feat_dim=128):\n",
    "        super(SupConEffNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(12, 36, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(36, 36, 3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.mybn1 = nn.BatchNorm2d(6)\n",
    "        self.mybn2 = nn.BatchNorm2d(12)\n",
    "        self.mybn3 = nn.BatchNorm2d(36)\n",
    "        self.mybn4 = nn.BatchNorm2d(36)\n",
    "        \n",
    "        dim_in = 1280\n",
    "        self.encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        self.encoder.conv_stem.weight = nn.Parameter(self.encoder.conv_stem.weight.repeat(1, 12, 1, 1))\n",
    "        self.encoder.classifier = nn.Identity()\n",
    "        \n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        x = F.relu6(self.mybn2(self.conv2(x)))\n",
    "        x = F.relu6(self.mybn3(self.conv3(x)))\n",
    "        x = F.relu6(self.mybn4(self.conv4(x)))\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "#         print(feat.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         print(feat.shape)\n",
    "        x = F.normalize(self.head(x), dim=1)\n",
    "#         print(feat.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa1c361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConEffNet_post(nn.Module):\n",
    "    \"\"\"backbone + projection head\n",
    "    + Post Stem Blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, head='mlp', feat_dim=128):\n",
    "        super(SupConEffNet_post, self).__init__()\n",
    "        \n",
    "        \n",
    "        dim_in = 1280\n",
    "        self.encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        self.encoder.conv_stem.stride = (1,1)\n",
    "        #self.encoder.conv_stem.weight = nn.Parameter(self.encoder.conv_stem.weight.repeat(1, 12, 1, 1))\n",
    "        self.encoder.classifier = nn.Identity()\n",
    "        num_channels = 32\n",
    "        self.post_stem = nn.ModuleList([timm.models.efficientnet_blocks.InvertedResidual(in_chs=num_channels, out_chs=num_channels, noskip=True),\n",
    "                    timm.models.efficientnet_blocks.InvertedResidual(in_chs=num_channels, out_chs=num_channels),\n",
    "                    timm.models.efficientnet_blocks.InvertedResidual(in_chs=num_channels, out_chs=num_channels),\n",
    "                    timm.models.efficientnet_blocks.InvertedResidual(in_chs=num_channels, out_chs=num_channels, stride=2)])\n",
    "        \n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                'head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.encoder.conv_stem(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        print(x.shape)\n",
    "        for idx, block in enumerate(self.post_stem):\n",
    "            x = block(x)\n",
    "        print(x.shape)\n",
    "        x = self.encoder.blocks(x)\n",
    "        x = self.encoder.conv_head(x)\n",
    "        x = self.encoder.bn2(x)\n",
    "        x = self.encoder.global_pool(x)\n",
    "#         print(feat.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         print(x.shape)\n",
    "        x = F.normalize(self.head(x), dim=1)\n",
    "#         print(feat.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aca9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBn(nn.Module):\n",
    "    \"\"\"Provides utility to create different types of layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            in_channels (int): no. of input channels.\n",
    "            out_channels (int): no. of output channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns Conv2d followed by BatchNorm.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of Conv2D -> BN.\n",
    "        \"\"\"\n",
    "        return self.batch_norm(self.conv(inp))\n",
    "\n",
    "\n",
    "class Type1(nn.Module):\n",
    "    \"\"\"Creates type 1 layer of SRNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.convbn = ConvBn(in_channels, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns type 1 layer of SRNet.\n",
    "        Args:\n",
    "            inp (Tensor): input tensor.\n",
    "        Returns:\n",
    "            Tensor: Output of type 1 layer.\n",
    "        \"\"\"\n",
    "        return self.relu(self.convbn(inp))\n",
    "\n",
    "\n",
    "class Type2(nn.Module):\n",
    "    \"\"\"Creates type 2 layer of SRNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.type1 = Type1(in_channels, out_channels)\n",
    "        self.convbn = ConvBn(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns type 2 layer of SRNet.\n",
    "        Args:\n",
    "            inp (Tensor): input tensor.\n",
    "        Returns:\n",
    "            Tensor: Output of type 2 layer.\n",
    "        \"\"\"\n",
    "        return inp + self.convbn(self.type1(inp))\n",
    "\n",
    "\n",
    "class Type3(nn.Module):\n",
    "    \"\"\"Creates type 3 layer of SRNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.type1 = Type1(in_channels, out_channels)\n",
    "        self.convbn = ConvBn(out_channels, out_channels)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns type 3 layer of SRNet.\n",
    "        Args:\n",
    "            inp (Tensor): input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of type 3 layer.\n",
    "        \"\"\"\n",
    "        out = self.batch_norm(self.conv1(inp))\n",
    "        out1 = self.pool(self.convbn(self.type1(inp)))\n",
    "        return out + out1\n",
    "\n",
    "\n",
    "class Type4(nn.Module):\n",
    "    \"\"\"Creates type 4 layer of SRNet.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.type1 = Type1(in_channels, out_channels)\n",
    "        self.convbn = ConvBn(out_channels, out_channels)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns type 4 layer of SRNet.\n",
    "        Args:\n",
    "            inp (Tensor): input tensor.\n",
    "        Returns:\n",
    "            Tensor: Output of type 4 layer.\n",
    "        \"\"\"\n",
    "        return self.gap(self.convbn(self.type1(inp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90300ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Srnet(nn.Module):\n",
    "    \"\"\"This is SRNet model class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "        super().__init__()\n",
    "        dim_in = 512\n",
    "        feat_dim = 128\n",
    "        self.type1s = nn.Sequential(Type1(3, 64), Type1(64, 16))\n",
    "        self.type2s = nn.Sequential(\n",
    "            Type2(16, 16),\n",
    "            Type2(16, 16),\n",
    "            Type2(16, 16),\n",
    "            Type2(16, 16),\n",
    "            Type2(16, 16),\n",
    "        )\n",
    "        self.type3s = nn.Sequential(\n",
    "            Type3(16, 16),\n",
    "            Type3(16, 64),\n",
    "            Type3(64, 128),\n",
    "            Type3(128, 256),\n",
    "        )\n",
    "        self.type4 = Type4(256, 512)\n",
    "        self.head = nn.Linear(dim_in, feat_dim)\n",
    "#         self.dense = nn.Linear(512, 2)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp: Tensor) -> Tensor:\n",
    "        \"\"\"Returns logits for input images.\n",
    "        Args:\n",
    "            inp (Tensor): input image tensor of shape (Batch, 1, 256, 256)\n",
    "        Returns:\n",
    "            Tensor: Logits of shape (Batch, 2)\n",
    "        \"\"\"\n",
    "        out = self.type1s(inp)\n",
    "        out = self.type2s(out)\n",
    "        out = self.type3s(out)\n",
    "        out = self.type4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.normalize(self.head(out), dim=1)\n",
    "#         out = self.dense(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d7e32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from components.attention import ChannelAttention, SpatialAttention, DualCrossModalAttention\n",
    "from components.srm_conv import SRMConv2d_simple, SRMConv2d_Separate\n",
    "from networks.xception import TransferModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "375806f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMPixelAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SRMPixelAttention, self).__init__()\n",
    "        self.srm = SRMConv2d_simple()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.pa = SpatialAttention()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=1)\n",
    "                if not m.bias is None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_srm = self.srm(x)\n",
    "        fea = self.conv(x_srm)        \n",
    "        att_map = self.pa(fea)\n",
    "        \n",
    "        return att_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6b5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_chan=2048*2, out_chan=2048, *args, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.convblk = nn.Sequential(\n",
    "            nn.Conv2d(in_chan, out_chan, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_chan),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ca = ChannelAttention(out_chan, ratio=16)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        fuse_fea = self.convblk(torch.cat((x, y), dim=1))\n",
    "        fuse_fea = fuse_fea + fuse_fea * self.ca(fuse_fea)\n",
    "        return fuse_fea\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None:\n",
    "                    nn.init.constant_(ly.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9f2e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_Stream_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xception_rgb = TransferModel(\n",
    "            'xception', dropout=0.5, inc=3, return_fea=True)\n",
    "        self.xception_srm = TransferModel(\n",
    "            'xception', dropout=0.5, inc=3, return_fea=True)\n",
    "\n",
    "        self.srm_conv0 = SRMConv2d_simple(inc=3)\n",
    "        self.srm_conv1 = SRMConv2d_Separate(32, 32)\n",
    "        self.srm_conv2 = SRMConv2d_Separate(64, 64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.att_map = None\n",
    "        self.srm_sa = SRMPixelAttention(3)\n",
    "        self.srm_sa_post = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dual_cma0 = DualCrossModalAttention(in_dim=728, ret_att=False)\n",
    "        self.dual_cma1 = DualCrossModalAttention(in_dim=728, ret_att=False)\n",
    "\n",
    "        self.fusion = FeatureFusionModule()\n",
    "        dim_in = 2048\n",
    "        feat_dim = 128\n",
    "        self.head = nn.Linear(dim_in, feat_dim)\n",
    "\n",
    "        self.att_dic = {}\n",
    "\n",
    "    def features(self, x):\n",
    "        srm = self.srm_conv0(x)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part1_0(x)\n",
    "        y = self.xception_srm.model.fea_part1_0(srm) \\\n",
    "            + self.srm_conv1(x)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part1_1(x)\n",
    "        y = self.xception_srm.model.fea_part1_1(y) \\\n",
    "            + self.srm_conv2(x)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        # srm guided spatial attention\n",
    "#         print(srm.shape)\n",
    "        self.att_map = self.srm_sa(srm)\n",
    "#         print(self.att_map)\n",
    "        x = x * self.att_map + x\n",
    "        x = self.srm_sa_post(x)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part2(x)\n",
    "        y = self.xception_srm.model.fea_part2(y)\n",
    "\n",
    "        x, y = self.dual_cma0(x, y)\n",
    "\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part3(x)        \n",
    "        y = self.xception_srm.model.fea_part3(y)\n",
    " \n",
    "\n",
    "        x, y = self.dual_cma1(x, y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part4(x)\n",
    "        y = self.xception_srm.model.fea_part4(y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part5(x)\n",
    "        y = self.xception_srm.model.fea_part5(y)\n",
    "\n",
    "        fea = self.fusion(x, y)\n",
    "                \n",
    "\n",
    "        return fea\n",
    "    def classifier(self, fea):\n",
    "        out, fea = self.xception_rgb.classifier(fea)\n",
    "        return out, fea\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: original rgb\n",
    "        '''\n",
    "        _ , fea = self.classifier(self.features(x))\n",
    "        fea = fea.view(x.size(0), -1)\n",
    "        fea = F.normalize(self.head(fea), dim=1)\n",
    "\n",
    "        return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da055b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7e2b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransFormer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransFormer, self).__init__()\n",
    "        # labels = ds['train'].features['labels'].names\n",
    "        model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=128)\n",
    "#         self.model.classifier = nn.Linear(768,128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.model(x)\n",
    "        fea = y.logits\n",
    "        fea = fea.view(x.size(0), -1)\n",
    "#         print(fea.shape)\n",
    "        fea = torch.nn.functional.normalize(fea, dim=1)\n",
    "        \n",
    "        return fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "353a184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "test_inp = torch.randn((1,3,224,224))\n",
    "test_model = TransFormer()\n",
    "test_out = test_model(test_inp)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98b2cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "#     model = SupConResNet(name='resnet50')\n",
    "    model = TransFormer()\n",
    "    criterion = SupConLoss(temperature=temp)\n",
    "\n",
    "    # enable synchronized Batch Normalization\n",
    "#     if opt.syncBN:\n",
    "#         model = apex.parallel.convert_syncbn_model(model)\n",
    "#         if torch.cuda.device_count() > 1:\n",
    "#             model.encoder = torch.nn.DataParallel(model.encoder)\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.cuda()\n",
    "#         cudnn.benchmark = True\n",
    "\n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "704734d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    \n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = torch.cat([images[0], images[1]], dim=0)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # warm-up learning rate\n",
    "#         warmup_learning_rate(epoch, idx, len(train_loader), optimizer)\n",
    "\n",
    "        # compute loss\n",
    "        features = model(images)\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "#         print(features.shape, labels.shape)\n",
    "#         break\n",
    "        if method == 'SupCon':\n",
    "            loss = criterion(features, labels)\n",
    "        elif method == 'SimCLR':\n",
    "            loss = criterion(features)\n",
    "        else:\n",
    "            raise ValueError('contrastive method not supported: {}'.\n",
    "                             format(method))\n",
    "\n",
    "        # update metric\n",
    "        losses.update(loss.item(), bsz)\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'loss {loss.val:.3f} ({loss.avg:.3f})'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2466cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107547\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.ConcatDataset([DF_train_dataset, F2F_train_dataset, FS_train_dataset, OR_train_dataset, OR_train_dataset, OR_train_dataset])\n",
    "# build data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1680aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # build model and criterion\n",
    "    model, criterion = set_model()\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of Network's Parameters: {total_params:,}\")\n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = set_optimizer(model)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                               max_lr=1e-3, \n",
    "                               epochs=epochs,\n",
    "                               steps_per_epoch=len(train_loader),\n",
    "                               pct_start=16.0/epochs,\n",
    "                               div_factor=25,\n",
    "                               final_div_factor=2)\n",
    "    model.train()\n",
    "\n",
    "    # tensorboard\n",
    "    writer = SummaryWriter(f'./runs')\n",
    "    print('Start Training')\n",
    "    # training routine\n",
    "    for epoch in range(1, epochs):\n",
    "#         adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        time1 = time.time()\n",
    "        loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "#         return 0\n",
    "        time2 = time.time()\n",
    "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "        print('epoch {}, loss {:.2f}'.format(epoch, loss))\n",
    "        print('*'*15)\n",
    "        \n",
    "        # tensorboard logger\n",
    "        writer.add_scalar('loss', loss, epoch)\n",
    "        writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if epoch % save_freq == 0:\n",
    "            save_file = os.path.join(\n",
    "                save_folder, 'ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "            save_model(model, optimizer, epoch, save_file)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(\n",
    "        save_folder, 'last.pth')\n",
    "    save_model(model, optimizer, epochs, save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "101465c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Network's Parameters: 85,897,088\n",
      "Start Training\n",
      "Train: [1][100/3361]\tBT 0.421 (0.436)\tDT 0.049 (0.053)\tloss 4.144 (4.240)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_642013/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_642013/804560799.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#         return 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_642013/217435498.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# measure elapsed time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/optim/adamax.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    137\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                    \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                    maximize=maximize)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/optim/adamax.py\u001b[0m in \u001b[0;36madamax\u001b[0;34m(params, grads, exp_avgs, exp_infs, state_steps, foreach, maximize, eps, beta1, beta2, lr, weight_decay)\u001b[0m\n\u001b[1;32m    187\u001b[0m          \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m          \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m          maximize=maximize)\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DatasetSobhanAsasi/vnev/lib/python3.7/site-packages/torch/optim/adamax.py\u001b[0m in \u001b[0;36m_single_tensor_adamax\u001b[0;34m(params, grads, exp_avgs, exp_infs, state_steps, eps, beta1, beta2, lr, weight_decay, maximize)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mexp_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         ], 0)\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnev",
   "language": "python",
   "name": "vnev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
