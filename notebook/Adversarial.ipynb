{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asasi\\anaconda3\\envs\\part_college\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch import Tensor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_freq = 100\n",
    "save_freq = 100\n",
    "batch_size = 64\n",
    "epochs = 400\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "lr_decay_epochs = \"100,200,300\"\n",
    "lr_decay_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# Dataset\n",
    "dataset = 'faceforensic'\n",
    "\n",
    "temp = 0.07\n",
    "\n",
    "#method\n",
    "method = 'Adv'\n",
    "\n",
    "cosine = True\n",
    "warm = True\n",
    "trial = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_24_2023_18_38_12\n"
     ]
    }
   ],
   "source": [
    "model_path = './save/Adv/{}_models'.format(dataset)\n",
    "tb_path = './save/Adv/{}_tensorboard'.format(dataset)\n",
    "\n",
    "save_time = f\"{datetime.datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}\"\n",
    "print(save_time)\n",
    "model_name = '{}_{}_{}_lr_{}_epochs_{}_bsz_{}'.\\\n",
    "        format(save_time, method, dataset, learning_rate,\n",
    "            epochs , batch_size)\n",
    "\n",
    "tb_folder = os.path.join(tb_path, model_name)\n",
    "if not os.path.isdir(tb_folder):\n",
    "    os.makedirs(tb_folder)\n",
    "\n",
    "save_folder = os.path.join(model_path, model_name)\n",
    "if not os.path.isdir(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]\n",
    "\n",
    "\n",
    "class AverageMeter():\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def accuracy_evaluate(output, target, topk=(1,)):\n",
    "    \"\"\"accuarcy for evaluation 3+1\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        correct = []\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tp = 0\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        pred = pred.view(-1)\n",
    "        # print(pred[5])\n",
    "        # print(target)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # print(pred[i].is_nonzero.eval())\n",
    "            # print(target[i].is_nonzero.eval())\n",
    "\n",
    "            if pred[i] == 0 and target[i] == 0:\n",
    "                correct.append(True)\n",
    "                tn += 1\n",
    "            elif pred[i] != 0 and target[i] != 0:\n",
    "                correct.append(True)\n",
    "                tp += 1\n",
    "            else:\n",
    "                if pred[i] == 0 and target[i] != 0:\n",
    "                    fn += 1\n",
    "                elif pred[i] != 0 and target[i] == 0:\n",
    "                    fp += 1\n",
    "                correct.append(False)\n",
    "\n",
    "        # print(correct)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = sum(bool(x) for x in correct)\n",
    "            # print(correct_k)\n",
    "            res.append(correct_k * 100.0 / batch_size)\n",
    "\n",
    "        return res\n",
    "\n",
    "def output_score(output, target):\n",
    "    print(output)\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if cosine:\n",
    "        eta_min = lr * (lr_decay_rate ** 3)\n",
    "        lr = eta_min + (lr - eta_min) * (\n",
    "                1 + math.cos(math.pi * epoch / epochs)) / 2\n",
    "    else:\n",
    "        steps = np.sum(epoch > np.asarray(lr_decay_epochs))\n",
    "        if steps > 0:\n",
    "            lr = lr * (lr_decay_rate ** steps)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def set_optimizer(model):\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=learning_rate,\n",
    "                          momentum=momentum,\n",
    "                          weight_decay=weight_decay)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adamax(model.parameters(),  lr=1e-3, eps=1e-4, weight_decay=1e-4)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, save_file):\n",
    "    print('==> Saving...')\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, save_file)\n",
    "    del state\n",
    "\n",
    "\n",
    "class LinearClassifierFeatureFusion(nn.Module):\n",
    "    \"\"\"Linear classifier\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(LinearClassifierFeatureFusion, self).__init__()\n",
    "        feat_dim = 3328\n",
    "        self.fc1 = nn.Linear(feat_dim, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = features.view(features.shape[0], -1)\n",
    "        x = self.fc1(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists('artifacts/train_idx.txt') == False:\n",
    "#     total_idx = list(range(999))\n",
    "#     random.shuffle(total_idx)\n",
    "#     train_idx = total_idx[:600]\n",
    "\n",
    "#     with open('train_idx.txt', 'w') as file:\n",
    "#         json.dump(train_idx, file)\n",
    "\n",
    "#     valid_idx = total_idx[600:800]\n",
    "\n",
    "#     with open('valid_idx.txt', 'w') as file:\n",
    "#         json.dump(valid_idx, file)\n",
    "\n",
    "\n",
    "#     test_idx = total_idx[800:]\n",
    "\n",
    "#     with open('test_idx.txt', 'w') as file:\n",
    "#         json.dump(test_idx, file)\n",
    "\n",
    "# else:\n",
    "\n",
    "with open('../artifacts/train_idx.txt', 'r') as file:\n",
    "    train_idx = json.load(file)\n",
    "    \n",
    "with open('../artifacts/valid_idx.txt', 'r') as file:\n",
    "    valid_idx = json.load(file)\n",
    "    \n",
    "with open('../artifacts/test_idx.txt', 'r') as file:\n",
    "    test_idx = json.load(file)\n",
    "        \n",
    "def idx_to_path(cat, indexes):\n",
    " \n",
    "    root = './dataset/dataset/Dataset/c23'\n",
    "    data_frame = {'path':[]}\n",
    "    path = root + '/{}'.format(cat)\n",
    "    for idx in indexes:\n",
    "        folder_path = path + '/{}'.format(idx)\n",
    "        if os.path.exists(folder_path) == True:\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = folder_path + '/{}'.format(file_name)\n",
    "                data_frame['path'].append(file_path)\n",
    "\n",
    "    if cat == 'Original':\n",
    "        labels = [1]*len(data_frame['path'])\n",
    "    else:\n",
    "        labels = [0]*len(data_frame['path'])\n",
    "\n",
    "    data_frame['labels'] = labels\n",
    "    data_frame = pd.DataFrame(data_frame)\n",
    "    \n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_data = idx_to_path('Deepfakes', train_idx)\n",
    "DF_valid_data = idx_to_path('Deepfakes', valid_idx)\n",
    "DF_test_data = idx_to_path('Deepfakes', test_idx)\n",
    "\n",
    "F2F_train_data = idx_to_path('Face2Face', train_idx)\n",
    "F2F_valid_data = idx_to_path('Face2Face', valid_idx)\n",
    "F2F_test_data = idx_to_path('Face2Face', test_idx)\n",
    "\n",
    "FS_train_data = idx_to_path('FaceSwap', train_idx)\n",
    "FS_valid_data = idx_to_path('FaceSwap', valid_idx)\n",
    "FS_test_data = idx_to_path('FaceSwap', test_idx)\n",
    "\n",
    "NT_train_data = idx_to_path('NeuralTextures', train_idx)\n",
    "NT_valid_data = idx_to_path('NeuralTextures', valid_idx)\n",
    "NT_test_data = idx_to_path('NeuralTextures', test_idx)\n",
    "\n",
    "OR_train_data = idx_to_path('Original', train_idx)\n",
    "OR_valid_data = idx_to_path('Original', valid_idx)\n",
    "OR_test_data = idx_to_path('Original', test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997\n",
      "17550\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_train_data))\n",
    "print(len(F2F_train_data))\n",
    "print(len(FS_train_data))\n",
    "print(len(OR_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "5880\n",
      "6000\n",
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_valid_data))\n",
    "print(len(F2F_valid_data))\n",
    "print(len(FS_valid_data))\n",
    "print(len(NT_valid_data))\n",
    "print(len(OR_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5970\n",
      "5730\n",
      "5970\n",
      "5970\n",
      "5970\n"
     ]
    }
   ],
   "source": [
    "print(len(DF_test_data))\n",
    "print(len(F2F_test_data))\n",
    "print(len(FS_test_data))\n",
    "print(len(NT_test_data))\n",
    "print(len(OR_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "    \n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_frame, cat,transform=None):\n",
    "        \n",
    "        self.data = data_frame\n",
    "        self.cat = cat\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.data.loc[idx, 'path']\n",
    "        label = self.data.loc[idx, 'labels']\n",
    "        # RGB\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train_dataset = MyDataset(DF_train_data, train_transform)\n",
    "DF_valid_dataset = MyDataset(DF_valid_data, train_transform)\n",
    "DF_test_dataset = MyDataset(DF_test_data, train_transform)\n",
    "\n",
    "F2F_train_dataset = MyDataset(F2F_train_data, train_transform)\n",
    "F2F_valid_dataset = MyDataset(F2F_valid_data, train_transform)\n",
    "F2F_test_dataset = MyDataset(F2F_test_data, train_transform)\n",
    "\n",
    "FS_train_dataset = MyDataset(FS_train_data, train_transform)\n",
    "FS_valid_dataset = MyDataset(FS_valid_data, train_transform)\n",
    "FS_test_dataset = MyDataset(FS_test_data, train_transform)\n",
    "\n",
    "NT_train_dataset = MyDataset(NT_train_data, train_transform)\n",
    "NT_valid_dataset = MyDataset(NT_valid_data, train_transform)\n",
    "NT_test_dataset = MyDataset(NT_test_data, train_transform)\n",
    "\n",
    "OR_train_dataset = MyDataset(OR_train_data, train_transform)\n",
    "OR_valid_dataset = MyDataset(OR_valid_data, train_transform)\n",
    "OR_test_dataset = MyDataset(OR_test_data, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMConv2d_simple(nn.Module):\n",
    "    \n",
    "    def __init__(self, inc=3, learnable=False):\n",
    "        super(SRMConv2d_simple, self).__init__()\n",
    "        self.truc = nn.Hardtanh(-3, 3)\n",
    "        kernel = self._build_kernel(inc)  # (3,3,5,5)\n",
    "        self.kernel = nn.Parameter(data=kernel, requires_grad=learnable)\n",
    "        # self.hor_kernel = self._build_kernel().transpose(0,1,3,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: imgs (Batch, H, W, 3)\n",
    "        '''\n",
    "        out = F.conv2d(x, self.kernel, stride=1, padding=2)\n",
    "        out = self.truc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _build_kernel(self, inc):\n",
    "        # filter1: KB\n",
    "        filter1 = [[0, 0, 0, 0, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 2, -4, 2, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 0, 0, 0, 0]]\n",
    "        # filter2：KV\n",
    "        filter2 = [[-1, 2, -2, 2, -1],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-2, 8, -12, 8, -2],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-1, 2, -2, 2, -1]]\n",
    "        # filter3：hor 2rd\n",
    "        filter3 = [[0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 1, -2, 1, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0]]\n",
    "\n",
    "        filter1 = np.asarray(filter1, dtype=float) / 4.\n",
    "        filter2 = np.asarray(filter2, dtype=float) / 12.\n",
    "        filter3 = np.asarray(filter3, dtype=float) / 2.\n",
    "        # statck the filters\n",
    "        filters = [[filter1],#, filter1, filter1],\n",
    "                   [filter2],#, filter2, filter2],\n",
    "                   [filter3]]#, filter3, filter3]]  # (3,3,5,5)\n",
    "        filters = np.array(filters)\n",
    "        filters = np.repeat(filters, inc, axis=1)\n",
    "        filters = torch.FloatTensor(filters)    # (3,3,5,5)\n",
    "        return filters\n",
    "\n",
    "class SRMConv2d_Separate(nn.Module):\n",
    "    \n",
    "    def __init__(self, inc, outc, learnable=False):\n",
    "        super(SRMConv2d_Separate, self).__init__()\n",
    "        self.inc = inc\n",
    "        self.truc = nn.Hardtanh(-3, 3)\n",
    "        kernel = self._build_kernel(inc)  # (3,3,5,5)\n",
    "        self.kernel = nn.Parameter(data=kernel, requires_grad=learnable)\n",
    "        # self.hor_kernel = self._build_kernel().transpose(0,1,3,2)\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.Conv2d(3*inc, outc, 1, 1, 0, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(outc),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        for ly in self.out_conv.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: imgs (Batch, H, W, 3)\n",
    "        '''\n",
    "        out = F.conv2d(x, self.kernel, stride=1, padding=2, groups=self.inc)\n",
    "        out = self.truc(out)\n",
    "        out = self.out_conv(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _build_kernel(self, inc):\n",
    "        # filter1: KB\n",
    "        filter1 = [[0, 0, 0, 0, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 2, -4, 2, 0],\n",
    "                   [0, -1, 2, -1, 0],\n",
    "                   [0, 0, 0, 0, 0]]\n",
    "        # filter2：KV\n",
    "        filter2 = [[-1, 2, -2, 2, -1],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-2, 8, -12, 8, -2],\n",
    "                   [2, -6, 8, -6, 2],\n",
    "                   [-1, 2, -2, 2, -1]]\n",
    "        # # filter3：hor 2rd\n",
    "        filter3 = [[0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 1, -2, 1, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0]]\n",
    "\n",
    "        filter1 = np.asarray(filter1, dtype=float) / 4.\n",
    "        filter2 = np.asarray(filter2, dtype=float) / 12.\n",
    "        filter3 = np.asarray(filter3, dtype=float) / 2.\n",
    "        # statck the filters\n",
    "        filters = [[filter1],#, filter1, filter1],\n",
    "                   [filter2],#, filter2, filter2],\n",
    "                   [filter3]]#, filter3, filter3]]  # (3,3,5,5)\n",
    "        filters = np.array(filters)\n",
    "        # filters = np.repeat(filters, inc, axis=1)\n",
    "        filters = np.repeat(filters, inc, axis=0)\n",
    "        filters = torch.FloatTensor(filters)    # (3,3,5,5)\n",
    "        # print(filters.size())\n",
    "        return filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.sharedMLP = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = self.sharedMLP(self.avg_pool(x))\n",
    "        maxout = self.sharedMLP(self.max_pool(x))\n",
    "        return self.sigmoid(avgout + maxout)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avgout, maxout], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The following modules are modified based on https://github.com/heykeetae/Self-Attention-GAN\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim=None, add=False, ratio=8):\n",
    "        super(Self_Attn, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.add = add\n",
    "        if out_dim is None:\n",
    "            out_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        # self.activation = activation\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=out_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(\n",
    "            m_batchsize, -1, width*height).permute(0, 2, 1)  # B X C X(N)\n",
    "        proj_key = self.key_conv(x).view(\n",
    "            m_batchsize, -1, width*height)  # B X C x (*W*H)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # transpose check\n",
    "        attention = self.softmax(energy)  # BX (N) X (N)\n",
    "        proj_value = self.value_conv(x).view(\n",
    "            m_batchsize, -1, width*height)  # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, self.out_dim, width, height)\n",
    "\n",
    "        if self.add:\n",
    "            out = self.gamma*out + x\n",
    "        else:\n",
    "            out = self.gamma*out\n",
    "        return out  # , attention\n",
    "\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\" CMA attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, activation=None, ratio=8, cross_value=True):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.cross_value = cross_value\n",
    "\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        proj_query = self.query_conv(x).view(\n",
    "            B, -1, H*W).permute(0, 2, 1)  # B , HW, C\n",
    "        proj_key = self.key_conv(y).view(\n",
    "            B, -1, H*W)  # B X C x (*W*H)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # B, HW, HW\n",
    "        attention = self.softmax(energy)  # BX (N) X (N)\n",
    "        if self.cross_value:\n",
    "            proj_value = self.value_conv(y).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "        else:\n",
    "            proj_value = self.value_conv(x).view(\n",
    "                B, -1, H*W)  # B , C , HW\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(out)\n",
    "\n",
    "        return out  # , attention\n",
    "\n",
    "\n",
    "class DualCrossModalAttention(nn.Module):\n",
    "    \"\"\" Dual CMA attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, activation=None, size=16, ratio=8, ret_att=False):\n",
    "        super(DualCrossModalAttention, self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        self.ret_att = ret_att\n",
    "\n",
    "        # query conv\n",
    "        self.key_conv1 = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv2 = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//ratio, kernel_size=1)\n",
    "        self.key_conv_share = nn.Conv2d(\n",
    "            in_channels=in_dim//ratio, out_channels=in_dim//ratio, kernel_size=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(size*size, size*size)\n",
    "        self.linear2 = nn.Linear(size*size, size*size)\n",
    "\n",
    "        # separated value conv\n",
    "        self.value_conv1 = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma1 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.value_conv2 = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma2 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        def _get_att(a, b):\n",
    "            proj_key1 = self.key_conv_share(self.key_conv1(a)).view(\n",
    "                B, -1, H*W).permute(0, 2, 1)  # B, HW, C\n",
    "            proj_key2 = self.key_conv_share(self.key_conv2(b)).view(\n",
    "                B, -1, H*W)  # B X C x (*W*H)\n",
    "            energy = torch.bmm(proj_key1, proj_key2)  # B, HW, HW\n",
    "\n",
    "            attention1 = self.softmax(self.linear1(energy))\n",
    "            attention2 = self.softmax(self.linear2(\n",
    "                energy.permute(0, 2, 1)))  # BX (N) X (N)\n",
    "\n",
    "            return attention1, attention2\n",
    "\n",
    "        att_y_on_x, att_x_on_y = _get_att(x, y)\n",
    "        proj_value_y_on_x = self.value_conv2(y).view(\n",
    "            B, -1, H*W)  # B, C, HW\n",
    "        out_y_on_x = torch.bmm(proj_value_y_on_x, att_y_on_x.permute(0, 2, 1))\n",
    "        out_y_on_x = out_y_on_x.view(B, C, H, W)\n",
    "        out_x = self.gamma1*out_y_on_x + x\n",
    "\n",
    "        proj_value_x_on_y = self.value_conv1(x).view(\n",
    "            B, -1, H*W)  # B , C , HW\n",
    "        out_x_on_y = torch.bmm(proj_value_x_on_y, att_x_on_y.permute(0, 2, 1))\n",
    "        out_x_on_y = out_x_on_y.view(B, C, H, W)\n",
    "        out_y = self.gamma2*out_x_on_y + y\n",
    "\n",
    "        if self.ret_att:\n",
    "            return out_x, out_y, att_y_on_x, att_x_on_y\n",
    "\n",
    "        return out_x, out_y  # , attention\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     x = torch.rand(10, 768, 16, 16)\n",
    "#     y = torch.rand(10, 768, 16, 16)\n",
    "#     dcma = DualCrossModalAttention(768, ret_att=True)\n",
    "#     out_x, out_y, att_y_on_x, att_x_on_y = dcma(x, y)\n",
    "#     print(out_y.size())\n",
    "#     print(att_x_on_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from https://github.com/ondyari/FaceForensics\n",
    "Author: Andreas Rössler\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import torch\n",
    "# import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.nets.xception import xception\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "# import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.nn import init\n",
    "\n",
    "pretrained_settings = {\n",
    "    'xception': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 299, 299],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5],\n",
    "            'num_classes': 1000,\n",
    "            'scale': 0.8975  # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETAINED_WEIGHT_PATH = '../src/components/networks/xception-b5690688.pth'\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
    "                               stride, padding, dilation, groups=in_channels, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, reps, strides=1, start_with_relu=True, grow_first=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if out_filters != in_filters or strides != 1:\n",
    "            self.skip = nn.Conv2d(in_filters, out_filters,\n",
    "                                  1, stride=strides, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = in_filters\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters,\n",
    "                                       3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "            filters = out_filters\n",
    "\n",
    "        for i in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(filters, filters,\n",
    "                                       3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d(in_filters, out_filters,\n",
    "                                       3, stride=1, padding=1, bias=False))\n",
    "            rep.append(nn.BatchNorm2d(out_filters))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        else:\n",
    "            rep[0] = nn.ReLU(inplace=False)\n",
    "\n",
    "        if strides != 1:\n",
    "            rep.append(nn.MaxPool2d(3, strides, 1))\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x += skip\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_gaussian_noise(ins, mean=0, stddev=0.2):\n",
    "    noise = ins.data.new(ins.size()).normal_(mean, stddev)\n",
    "    return ins + noise\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Xception optimized for the ImageNet dataset, as specified in\n",
    "    https://arxiv.org/pdf/1610.02357.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1000, inc=3):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            num_classes: number of classes\n",
    "        \"\"\"\n",
    "        super(Xception, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Entry flow\n",
    "        self.conv1 = nn.Conv2d(inc, 32, 3, 2, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # do relu here\n",
    "\n",
    "        self.block1 = Block(\n",
    "            64, 128, 2, 2, start_with_relu=False, grow_first=True)\n",
    "        self.block2 = Block(\n",
    "            128, 256, 2, 2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(\n",
    "            256, 728, 2, 2, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        # middle flow\n",
    "        self.block4 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block5 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block6 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block7 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        self.block8 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block9 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block10 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "        self.block11 = Block(\n",
    "            728, 728, 3, 1, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        # Exit flow\n",
    "        self.block12 = Block(\n",
    "            728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n",
    "\n",
    "        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        # do relu here\n",
    "        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "        # #------- init weights --------\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        #     elif isinstance(m, nn.BatchNorm2d):\n",
    "        #         m.weight.data.fill_(1)\n",
    "        #         m.bias.data.zero_()\n",
    "        # #-----------------------------\n",
    "    def fea_part1_0(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part1_1(self, x):\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part1(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part2(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part3(self, x):\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part4(self, x):\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fea_part5(self, x):\n",
    "        x = self.block12(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def features(self, input):\n",
    "        x = self.fea_part1(input)\n",
    "\n",
    "        x = self.fea_part2(x)\n",
    "        x = self.fea_part3(x)\n",
    "        x = self.fea_part4(x)\n",
    "\n",
    "        x = self.fea_part5(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, features):\n",
    "        x = self.relu(features)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.last_linear(x)\n",
    "        return out, x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        out, x = self.classifier(x)\n",
    "        return out, x\n",
    "\n",
    "\n",
    "def xception(num_classes=1000, pretrained='imagenet', inc=3):\n",
    "    model = Xception(num_classes=num_classes, inc=inc)\n",
    "    if pretrained:\n",
    "        settings = pretrained_settings['xception'][pretrained]\n",
    "        assert num_classes == settings['num_classes'], \\\n",
    "            \"num_classes should be {}, but is {}\".format(\n",
    "                settings['num_classes'], num_classes)\n",
    "\n",
    "        model = Xception(num_classes=num_classes)\n",
    "        model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "\n",
    "        model.input_space = settings['input_space']\n",
    "        model.input_size = settings['input_size']\n",
    "        model.input_range = settings['input_range']\n",
    "        model.mean = settings['mean']\n",
    "        model.std = settings['std']\n",
    "\n",
    "    # TODO: ugly\n",
    "    model.last_linear = model.fc\n",
    "    del model.fc\n",
    "    return model\n",
    "\n",
    "\n",
    "class TransferModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple transfer learning model that takes an imagenet pretrained model with\n",
    "    a fc layer as base model and retrains a new fc layer for num_out_classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0,\n",
    "                 weight_norm=False, return_fea=False, inc=3):\n",
    "        super(TransferModel, self).__init__()\n",
    "        self.modelchoice = modelchoice\n",
    "        self.return_fea = return_fea\n",
    "\n",
    "        if modelchoice == 'xception':\n",
    "\n",
    "            def return_pytorch04_xception(pretrained=True):\n",
    "                # Raises warning \"src not broadcastable to dst\" but thats fine\n",
    "                model = xception(pretrained=False)\n",
    "                if pretrained:\n",
    "                    # Load model in torch 0.4+\n",
    "                    model.fc = model.last_linear\n",
    "                    del model.last_linear\n",
    "                    state_dict = torch.load(\n",
    "                        PRETAINED_WEIGHT_PATH)\n",
    "                    for name, weights in state_dict.items():\n",
    "                        if 'pointwise' in name:\n",
    "                            state_dict[name] = weights.unsqueeze(\n",
    "                                -1).unsqueeze(-1)\n",
    "                    model.load_state_dict(state_dict)\n",
    "                    model.last_linear = model.fc\n",
    "                    del model.fc\n",
    "                return model\n",
    "\n",
    "            self.model = return_pytorch04_xception()\n",
    "            # Replace fc\n",
    "            num_ftrs = self.model.last_linear.in_features\n",
    "            if not dropout:\n",
    "                if weight_norm:\n",
    "                    print('Using Weight_Norm')\n",
    "                    self.model.last_linear = nn.utils.weight_norm(\n",
    "                        nn.Linear(num_ftrs, num_out_classes), name='weight')\n",
    "                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n",
    "            else:\n",
    "                print('Using dropout', dropout)\n",
    "                if weight_norm:\n",
    "                    print('Using Weight_Norm')\n",
    "                    self.model.last_linear = nn.Sequential(\n",
    "                        nn.Dropout(p=dropout),\n",
    "                        nn.utils.weight_norm(\n",
    "                            nn.Linear(num_ftrs, num_out_classes), name='weight')\n",
    "                    )\n",
    "\n",
    "                self.model.last_linear = nn.Sequential(\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    nn.Identity()\n",
    "                )\n",
    "\n",
    "            if inc != 3:\n",
    "                self.model.conv1 = nn.Conv2d(inc, 32, 3, 2, 0, bias=False)\n",
    "                nn.init.xavier_normal(self.model.conv1.weight.data, gain=0.02)\n",
    "\n",
    "        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n",
    "            if modelchoice == 'resnet50':\n",
    "                self.model = torchvision.models.resnet50(pretrained=True)\n",
    "            if modelchoice == 'resnet18':\n",
    "                self.model = torchvision.models.resnet18(pretrained=True)\n",
    "            # Replace fc\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "            if not dropout:\n",
    "                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n",
    "            else:\n",
    "                self.model.fc = nn.Sequential(\n",
    "                    nn.Dropout(p=dropout),\n",
    "                    nn.Linear(num_ftrs, num_out_classes)\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('Choose valid model, e.g. resnet50')\n",
    "\n",
    "    def set_trainable_up_to(self, boolean=False, layername=\"Conv2d_4a_3x3\"):\n",
    "        \"\"\"\n",
    "        Freezes all layers below a specific layer and sets the following layers\n",
    "        to true if boolean else only the fully connected final layer\n",
    "        :param boolean:\n",
    "        :param layername: depends on lib, for inception e.g. Conv2d_4a_3x3\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Stage-1: freeze all the layers\n",
    "        if layername is None:\n",
    "            for i, param in self.model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "                return\n",
    "        else:\n",
    "            for i, param in self.model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "        if boolean:\n",
    "            # Make all layers following the layername layer trainable\n",
    "            ct = []\n",
    "            found = False\n",
    "            for name, child in self.model.named_children():\n",
    "                if layername in ct:\n",
    "                    found = True\n",
    "                    for params in child.parameters():\n",
    "                        params.requires_grad = True\n",
    "                ct.append(name)\n",
    "            if not found:\n",
    "                raise NotImplementedError('Layer not found, cant finetune!'.format(\n",
    "                    layername))\n",
    "        else:\n",
    "            if self.modelchoice == 'xception':\n",
    "                # Make fc trainable\n",
    "                for param in self.model.last_linear.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            else:\n",
    "                # Make fc trainable\n",
    "                for param in self.model.fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, x = self.model(x)\n",
    "        if self.return_fea:\n",
    "            return out, x\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.model.features(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        out, x = self.model.classifier(x)\n",
    "        return out, x\n",
    "\n",
    "\n",
    "def model_selection(modelname, num_out_classes,\n",
    "                    dropout=None):\n",
    "    \"\"\"\n",
    "    :param modelname:\n",
    "    :return: model, image size, pretraining<yes/no>, input_list\n",
    "    \"\"\"\n",
    "    if modelname == 'xception':\n",
    "        return TransferModel(modelchoice='xception',\n",
    "                             num_out_classes=num_out_classes), 299, \\\n",
    "            True, ['image'], None\n",
    "    elif modelname == 'resnet18':\n",
    "        return TransferModel(modelchoice='resnet18', dropout=dropout,\n",
    "                             num_out_classes=num_out_classes), \\\n",
    "            224, True, ['image'], None\n",
    "    else:\n",
    "        raise NotImplementedError(modelname)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     model = TransferModel('xception', dropout=0.5)\n",
    "#     print(model)\n",
    "#     # model = model.cuda()\n",
    "#     # from torchsummary import summary\n",
    "#     # input_s = (3, image_size, image_size)\n",
    "#     # print(summary(model, input_s))\n",
    "#     dummy = torch.rand(10, 3, 256, 256)\n",
    "#     out = model(dummy)\n",
    "#     print(out.size())\n",
    "#     x = model.features(dummy)\n",
    "#     out, x = model.classifier(x)\n",
    "#     print(out.size())\n",
    "#     print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMPixelAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SRMPixelAttention, self).__init__()\n",
    "        self.srm = SRMConv2d_simple()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.pa = SpatialAttention()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=1)\n",
    "                if not m.bias is None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_srm = self.srm(x)\n",
    "        fea = self.conv(x_srm)        \n",
    "        att_map = self.pa(fea)\n",
    "        \n",
    "        return att_map\n",
    "\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_chan=2048*2, out_chan=2048, *args, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.convblk = nn.Sequential(\n",
    "            nn.Conv2d(in_chan, out_chan, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_chan),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ca = ChannelAttention(out_chan, ratio=16)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        fuse_fea = self.convblk(torch.cat((x, y), dim=1))\n",
    "        fuse_fea = fuse_fea + fuse_fea * self.ca(fuse_fea)\n",
    "        return fuse_fea\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None:\n",
    "                    nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "\n",
    "class Two_Stream_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.xception_rgb = TransferModel(\n",
    "            'xception', dropout=0.5, inc=3, return_fea=True)\n",
    "        self.xception_srm = TransferModel(\n",
    "            'xception', dropout=0.5, inc=3, return_fea=True)\n",
    "\n",
    "        self.srm_conv0 = SRMConv2d_simple(inc=3)\n",
    "        self.srm_conv1 = SRMConv2d_Separate(32, 32)\n",
    "        self.srm_conv2 = SRMConv2d_Separate(64, 64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.att_map = None\n",
    "        self.srm_sa = SRMPixelAttention(3)\n",
    "        self.srm_sa_post = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dual_cma0 = DualCrossModalAttention(in_dim=728, ret_att=False)\n",
    "        self.dual_cma1 = DualCrossModalAttention(in_dim=728, ret_att=False)\n",
    "\n",
    "        self.fusion = FeatureFusionModule()\n",
    "\n",
    "        self.att_dic = {}\n",
    "\n",
    "    def features(self, x):\n",
    "        srm = self.srm_conv0(x)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part1_0(x)\n",
    "        y = self.xception_srm.model.fea_part1_0(srm) \\\n",
    "            + self.srm_conv1(x)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part1_1(x)\n",
    "        y = self.xception_srm.model.fea_part1_1(y) \\\n",
    "            + self.srm_conv2(x)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        # srm guided spatial attention\n",
    "        self.att_map = self.srm_sa(srm)\n",
    "        x = x * self.att_map + x\n",
    "        x = self.srm_sa_post(x)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part2(x)\n",
    "        y = self.xception_srm.model.fea_part2(y)\n",
    "\n",
    "        x, y = self.dual_cma0(x, y)\n",
    "\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part3(x)        \n",
    "        y = self.xception_srm.model.fea_part3(y)\n",
    " \n",
    "\n",
    "        x, y = self.dual_cma1(x, y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part4(x)\n",
    "        y = self.xception_srm.model.fea_part4(y)\n",
    "\n",
    "        x = self.xception_rgb.model.fea_part5(x)\n",
    "        y = self.xception_srm.model.fea_part5(y)\n",
    "\n",
    "        fea = self.fusion(x, y)\n",
    "                \n",
    "\n",
    "        return fea\n",
    "\n",
    "    def classifier(self, fea):\n",
    "        out, fea = self.xception_rgb.classifier(fea)\n",
    "        return out, fea\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: original rgb\n",
    "        '''\n",
    "        out, fea = self.classifier(self.features(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dropout 0.5\n",
      "Using dropout 0.5\n",
      "torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "backbone = Two_Stream_Net()\n",
    "dummy = torch.randn((1,3,256,256))\n",
    "out = backbone(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.backbone = backbone\n",
    "    self.network = nn.Sequential(\n",
    "                    nn.Linear(2048, 512),\n",
    "                    nn.Linear(512,64),\n",
    "                    nn.Linear(64,2)\n",
    "                )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.backbone(x)\n",
    "    # print(x.shape)\n",
    "    out = self.network(x)\n",
    "\n",
    "    return out\n",
    "\n",
    "class Domain_Generalize(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "      self.backbone = backbone\n",
    "      self.network = nn.Sequential(\n",
    "                    nn.Linear(2048, 3),\n",
    "                )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.backbone(x)\n",
    "    # print(x.shape)\n",
    "    out = self.network(x)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "net1 = Classify()\n",
    "out1 = net1(dummy)\n",
    "print(out1.shape)\n",
    "\n",
    "net2 = Domain_Generalize()\n",
    "out2 = net2(dummy)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    cls_model = net1.to(device)\n",
    "    gen_model = net2.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    return cls_model, gen_model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer(cls_model, gen_model):\n",
    "\n",
    "    optim1 = optim.Adamax(cls_model.parameters(),  lr=1e-3, eps=1e-4, weight_decay=1e-4)\n",
    "    optim2 = optim.Adamax(gen_model.parameters(),  lr=1e-3, eps=1e-4, weight_decay=1e-4)\n",
    "    return optim1, optim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, cls_model, gen_model, criterion, optim1, optim2, epoch):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    \n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    gen_losses = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # compute classification loss\n",
    "        out1 = cls_model(images)\n",
    "        loss1 = criterion(out1, labels)\n",
    "        cls_losses.update(loss1.item(), bsz)\n",
    "\n",
    "        optim1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optim1.step()\n",
    "\n",
    "        # compute generalization loss\n",
    "        fake_images = images[labels == 0]\n",
    "        out2 = gen_model(fake_images)\n",
    "        fake_labels = torch.ones_like(out2) / 3\n",
    "        loss2 = criterion(out2, fake_labels)\n",
    "        gen_losses.update(loss2.item(), bsz)\n",
    "\n",
    "        optim2.zero_grad()\n",
    "        loss2.backward()\n",
    "        optim2.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'cls_loss {cls_loss.val:.3f} ({cls_loss.avg:.3f})\\t'\n",
    "                  'gen_loss {gen_loss.val:.3f} ({gen_loss.avg:.3f})\\t'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, cls_loss=cls_losses, gen_loss=gen_losses))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return cls_losses.avg, gen_losses.avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, cls_model, gen_model, criterion, epoch):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "    \n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    gen_losses = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, labels) in enumerate(test_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bsz = labels.shape[0]\n",
    "\n",
    "        # compute classification loss\n",
    "        out1 = cls_model(images)\n",
    "        loss1 = criterion(out1, labels)\n",
    "        cls_losses.update(loss1.item(), bsz)\n",
    "\n",
    "        # compute generalization loss\n",
    "        fake_images = images[labels == 0]\n",
    "        out2 = gen_model(fake_images)\n",
    "        fake_labels = torch.ones_like(out2) / 3\n",
    "        loss2 = criterion(out2, fake_labels)\n",
    "        gen_losses.update(loss2.item(), bsz)\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # print info\n",
    "        if (idx + 1) % print_freq == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'cls_loss {cls_loss.val:.3f} ({cls_loss.avg:.3f})\\t'\n",
    "                  'gen_loss {gen_loss.val:.3f} ({gen_loss.avg:.3f})\\t'.format(\n",
    "                   epoch, idx + 1, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, cls_loss=cls_losses, gen_loss=gen_losses))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return cls_losses.avg, gen_losses.avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107547\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.ConcatDataset([DF_train_dataset, F2F_train_dataset, FS_train_dataset, OR_train_dataset, OR_train_dataset, OR_train_dataset])\n",
    "valid_dataset = torch.utils.data.ConcatDataset([DF_valid_dataset, F2F_valid_dataset, FS_valid_dataset, OR_valid_dataset, OR_valid_dataset, OR_valid_dataset])\n",
    "# build data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # build model and criterion\n",
    "    cls_model, gen_model, criterion = set_model()\n",
    "    \n",
    "    total_params = sum(p.numel() for p in cls_model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of Classififcation Network's Parameters: {total_params:,}\")\n",
    "    total_params = sum(p.numel() for p in gen_model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of Classififcation Network's Parameters: {total_params:,}\")\n",
    "\n",
    "    # build optimizer\n",
    "    optim1, optim2 = set_optimizer(cls_model, gen_model)\n",
    "    scheduler1 = optim.lr_scheduler.OneCycleLR(optim1, \n",
    "                               max_lr=1e-3, \n",
    "                               epochs=epochs,\n",
    "                               steps_per_epoch=len(train_loader),\n",
    "                               pct_start=16.0/epochs,\n",
    "                               div_factor=25,\n",
    "                               final_div_factor=2)\n",
    "    scheduler2 = optim.lr_scheduler.OneCycleLR(optim2, \n",
    "                               max_lr=1e-3, \n",
    "                               epochs=epochs,\n",
    "                               steps_per_epoch=len(train_loader),\n",
    "                               pct_start=16.0/epochs,\n",
    "                               div_factor=25,\n",
    "                               final_div_factor=2)\n",
    "\n",
    "    # tensorboard\n",
    "    writer = SummaryWriter(tb_folder)\n",
    "    print('Start Training')\n",
    "    # training routine\n",
    "    for epoch in range(1, epochs):\n",
    "#         adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        time1 = time.time()\n",
    "        cls_model.train()\n",
    "        gen_model.train()\n",
    "        cls_loss, gen_loss = train(train_loader, cls_model, gen_model, criterion, optim1, optim2, epoch)\n",
    "        scheduler1.step()\n",
    "        scheduler2.step()\n",
    "#         return 0\n",
    "        time2 = time.time()\n",
    "        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))\n",
    "        print('epoch {}, cls loss {:.2f}, gen loss {:.2f}'.format(epoch, cls_loss, gen_loss))\n",
    "        print('*'*15)\n",
    "        \n",
    "        # tensorboard logger\n",
    "        writer.add_scalar('cls loss', cls_loss, epoch)\n",
    "        writer.add_scalar('gen loss', gen_loss, epoch)\n",
    "        writer.add_scalar('cls learning_rate', optim1.param_groups[0]['lr'], epoch)\n",
    "        writer.add_scalar('gen learning_rate', optim2.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if epoch % save_freq == 0:\n",
    "            save_file = os.path.join(\n",
    "                save_folder, 'cls_ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "            save_model(cls_model, optim1, epoch, save_file)\n",
    "            save_file = os.path.join(\n",
    "                save_folder, 'gen_ckpt_epoch_{epoch}.pth'.format(epoch=epoch))\n",
    "            save_model(gen_model, optim2, epoch, save_file)\n",
    "    \n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(\n",
    "        save_folder, 'cls_last.pth')\n",
    "    save_model(cls_model, optim1, epochs, save_file)\n",
    "    save_file = os.path.join(\n",
    "        save_folder, 'gen_last.pth')\n",
    "    save_model(gen_model, optim2, epochs, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part_college",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
